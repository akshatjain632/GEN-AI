{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFEUuGXNNSGB"
      },
      "source": [
        "# RNN (Graded)\n",
        "\n",
        "Welcome to your RNN (required) programming assignment! You will build a **Recurrent Neural Network(RNN)** for **text completion** task. You will be using [WikiText language modeling dataset](https://huggingface.co/datasets/Salesforce/wikitext) which contains million of tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
        "\n",
        "**Instructions:**\n",
        "* Do not modify any of the codes.\n",
        "* Only write code when prompted. For example in some sections you will find the following,\n",
        "```\n",
        "# TODO\n",
        "```\n",
        "Only modify those sections of the code.\n",
        "And follow the instructions in the code cell where you need to write code.\n",
        "\n",
        "**You will learn to:**\n",
        "* Explore the [WikiText language modeling dataset](https://huggingface.co/datasets/Salesforce/wikitext) dataset.\n",
        "* Clean the dataset before using it for training.\n",
        "* Build a robust text completion model using just `SimpleRNN()`.\n",
        "* Build the general architecture of a RNN, including:\n",
        "  * Initializing parameters\n",
        "  * Calculating the cost function and its gradient\n",
        "  * Using an optimization algorithm (gradient descent)\n",
        "* Gather all three functions above into a main model function, in the right order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2x3zjsreV0a0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "from helpers import *\n",
        "from tests import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx9CPRkQGweR"
      },
      "source": [
        "# Loading and Visualizing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarrow in c:\\users\\arj64\\anaconda3\\lib\\site-packages (19.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pyarrow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gAcdiWdOHPLD"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d0be94eaa514f92bca7d0496753656f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/802 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arj64\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arj64\\.cache\\huggingface\\hub\\datasets--iohadrubin--wikitext-103-raw-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42bf5798602d47ee96edfcc86ebce0da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00000-of-00002-b755d19de94348c6.parquet:   0%|          | 0.00/148M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8aa58a18e0c54ddc970a431c3f629909",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00001-of-00002-0bf6d0c487c2e75b.parquet:   0%|          | 0.00/152M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "178c825cfe5f4de2b169ce76162a2d9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00000-of-00001-4c013962448951dd.parquet:   0%|          | 0.00/631k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57b403621d054eb98dbd41ae966de2d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00000-of-00001-b7859cf6365689a3.parquet:   0%|          | 0.00/707k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b19a0bffa724fb7af8660f66d205e57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/29567 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deacb4a5dcc040598ecf48620787172a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ec4b95e615844dab38fd0d58f73e798",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/62 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"train\")\n",
        "valid_dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"validation\")\n",
        "test_dataset  = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kFTG-DSKQkZm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 29567\n",
            "Valid dataset size: 60\n",
            "Test dataset size: 62\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Valid dataset size: {len(valid_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mYIq6yytK-M7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'= Valkyria Chronicles III =\\nSenjō no Valkyria 3: Unrecorded Chronicles (Japanese: 戦場のヴァルキュリア3, lit. Valkyria of the Battlefield 3), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role-playing video game developed by Sega and Media.Vision for the PlayStation Portable. Released in January 2011 in Japan, it is the third game in the Valkyria series. Employing the same fusion of tactical and real-time gameplay as its predecessors, the story runs parallel to the first game and follows the \"Nameless\", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \"Calamaty Raven\".\\nThe game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Ozawa. A large team of writers handled the script. The game\\'s opening theme was sung by May\\'n.\\nIt met with positive sales in Japan, and was praised by both Japanese and western critics. After release, it received downloadable content, along with an expanded edition in November of that year. It was also adapted into manga and an original video animation series. Due to low sales of Valkyria Chronicles II, Valkyria Chronicles III was not localized, but a fan translation compatible with the game\\'s expanded edition was released in 2014. Media.Vision would return to the franchise with the development of Valkyria: Azure Revolution for the PlayStation 4.\\n= = Gameplay = =\\nAs with previous Valkyira Chronicles games, Valkyria Chronicles III is a tactical role-playing game where players take control of a military unit and take part in missions against enemy forces. Stories are told through comic book-like panels with animated character portraits, with characters speaking partially through voiced speech bubbles and partially through unvoiced text. The player progresses through a series of linear missions, gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked. The route to each story location on the map varies depending on an individual player\\'s approach: when one option is selected, the other is sealed off to the player. Outside missions, the player characters rest in a camp, where units can be customized and character growth occurs. Alongside the main story missions are character-specific sub missions relating to different squad members. After the game\\'s completion, additional episodes are unlocked, some of them having a higher difficulty than those found in the rest of the game. There are also love simulation elements related to the game\\'s two main heroines, although they take a very minor role.\\nThe game\\'s battle system, the BliTZ system, is carried over directly from Valkyira Chronicles. During missions, players select each unit using a top-down perspective of the battlefield map: once a character is selected, the player moves the character around the battlefield in third-person. A character can only act once per-turn, but characters can be granted multiple turns at the expense of other characters\\' turns. Each character has a field and distance of movement limited by their Action Gauge. Up to nine characters can be assigned to a single mission. During gameplay, characters will call out if something happens to them, such as their health points (HP) getting low or being knocked out by enemy attacks. Each character has specific \"Potentials\", skills unique to each character. They are divided into \"Personal Potential\", which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character, and \"Battle Potentials\", which are grown throughout the game and always grant boons to a character. To learn Battle Potentials, each character has a unique \"Masters Table\", a grid-based skill table that can be used to acquire and link different skills. Characters also have Special Abilities that grant them temporary boosts on the battlefield: Kurt can activate \"Direct Command\" and move around the battlefield without depleting his Action Point gauge, the character Reila can shift into her \"Valkyria Form\" and become invincible, while Imca can target multiple enemy units with her heavy weapon.\\nTroops are divided into five classes: Scouts, Shocktroopers, Engineers, Lancers and Armored Soldier. Troopers can switch classes by changing their assigned weapon. Changing class does not greatly affect the stats gained while in a previous class. With victory in battle, experience points are awarded to the squad, which are distributed into five different attributes shared by the entire squad, a feature differing from early games\\' method of distributing to different unit types.\\n= = Plot = =\\nThe game takes place during the Second Europan War. Gallian Army Squad 422, also known as \"The Nameless\", are a penal military unit composed of criminals, foreign deserters, and military offenders whose real names are erased from the records and thereon officially referred to by numbers. Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do, they are nevertheless up to the task, exemplified by their motto, Altaha Abilia, meaning \"Always Ready.\" The three main characters are No.7 Kurt Irving, an army officer falsely accused of treason who wishes to redeem himself; Ace No.1 Imca, a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home; and No.13 Riela Marcellis, a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria. Together with their fellow squad members, these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven, consisting of mostly Darcsen soldiers.\\nAs the Nameless officially do not exist, the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war. While at times this works to their advantage, such as a successful incursion into Imperial territory, other orders cause certain members of the 422nd great distress. One such member, Gusurg, becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven, attached to the ideal of Darcsen independence proposed by their leader, Dahau. At the same time, elements within Gallian Army Command move to erase the Nameless in order to protect their own interests. Hounded by both allies and enemies, and combined with the presence of a traitor within their ranks, the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort. This continues until the Nameless\\'s commanding officer, Ramsey Crowe, who had been kept under house arrest, is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor, the Gallian General that had accused Kurt of Treason.\\nPartly due to these events, and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire, the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force. This is short-lived, however, as following Maximilian\\'s defeat, Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire, kept secret by their benefactor. Without the support of Maximilian or the chance to prove themselves in the war with Gallia, it is Dahau\\'s last trump card in creating a new Darcsen nation. As an armed Gallian force invading the Empire just following the two nations\\' cease-fire would certainly wreck their newfound peace, Kurt decides to once again make his squad the Nameless, asking Crowe to list himself and all under his command as killed-in-action. Now owing allegiance to none other than themselves, the 422nd confronts Dahau and destroys the Valkyrian weapon. Each member then goes their separate ways in order to begin their lives anew.\\n= = Development = =\\nConcept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010, with full development beginning shortly after this. The director of Valkyria Chronicles II, Takeshi Ozawa, returned to that role for Valkyria Chronicles III. Development work took approximately one year. After the release of Valkyria Chronicles II, the staff took a look at both the popular response for the game and what they wanted to do next for the series. Like its predecessor, Valkyria Chronicles III was developed for PlayStation Portable: this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II, and they had not come up with the \"revolutionary\" idea that would warrant a new entry for the PlayStation 3. Speaking in an interview, it was stated that the development team considered Valkyria Chronicles III to be the series\\' first true sequel: while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move, the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform. In addition to Sega staff from the previous games, development work was also handled by Media.Vision. The original scenario was written Kazuki Yamanobe, while the script was written by Hiroyuki Fujii, Koichi Majima, Kishiko Miyagi, Seiki Nagakawa and Takayuki Shouji. Its story was darker and more somber than that of its predecessor.\\nThe majority of material created for previous games, such as the BLiTZ system and the design of maps, was carried over. Alongside this, improvements were made to the game\\'s graphics and some elements were expanded, such as map layouts, mission structure, and the number of playable units per mission. A part of this upgrade involved creating unique polygon models for each character\\'s body. In order to achieve this, the cooperative elements incorporated into the second game were removed, as they took up a large portion of memory space needed for the improvements. They also adjusted the difficulty settings and ease of play so they could appeal to new players while retaining the essential components of the series\\' gameplay. The newer systems were decided upon early in development. The character designs were done by Raita Honjou, who had worked on the previous Valkyria Chronicles games. When creating the Nameless Squad, Honjou was faced with the same problem he had had during the first game: the military uniforms essentially destroyed character individuality, despite him needing to create unique characters the player could identify while maintaining a sense of reality within the Valkyria Chronicles world. The main color of the Nameless was black. As with the previous Valkyria games, Valkyria Chronicles III used the CANVAS graphics engine. The anime opening was produced by Production I.G.\\n= = = Music = = =\\nThe music was composed by Hitoshi Sakimoto, who had also worked on the previous Valkyria Chronicles games. When he originally heard about the project, he thought it would be a light tone similar to other Valkyria Chronicles games, but found the themes much darker than expected. An early theme he designed around his original vision of the project was rejected. He redid the main theme about seven times through the music production due to this need to reassess the game. The main theme was initially recorded using orchestra, then Sakimoto removed elements such as the guitar and bass, then adjusted the theme using a synthesizer before redoing segments such as the guitar piece on their own before incorporating them into the theme. The rejected main theme was used as a hopeful tune that played during the game\\'s ending. The battle themes were designed around the concept of a \"modern battle\" divorced from a fantasy scenario by using modern musical instruments, constructed to create a sense of atonality. While Sakimoto was most used to working with synthesized music, he felt that he needed to incorporate live instruments such as orchestra and guitar. The guitar was played by Mitsuhiro Ohta, who also arranged several of the later tracks. The game\\'s opening theme song, \"If You Wish for...\" (もしも君が願うのなら, Moshimo Kimi ga Negauno Nara), was sung by Japanese singer May\\'n. Its theme was the reason soldiers fought, in particular their wish to protect what was precious to them rather than a sense of responsibility or duty. Its lyrics were written by Seiko Fujibayashi, who had worked on May\\'n on previous singles.\\n= = = Release = = =\\nIn September 2010, a teaser website was revealed by Sega, hinting at a new Valkyria Chronicles game. In its September issue, Famitsu listed that Senjō no Valkyria 3 would be arriving on the PlayStation Portable. Its first public appearance was at the 2010 Tokyo Game Show (TGS), where a demo was made available for journalists and attendees. During the publicity, story details were kept scant so as not to spoil too much for potential players, along with some of its content still being in flux at the time of its reveal. To promote the game and detail the story leading into the game\\'s events, an episodic Flash visual novel written by Fujii began release in January 2011. The game was released January 27, 2011. During an interview, the development team said that the game had the capacity for downloadable content (DLC), but that no plans were finalized. Multiple DLC maps, featuring additional missions and recruitable characters, were released between February and April 2011. An expanded edition of the game, Valkyria Chronicles III Extra Edition, released on November 23, 2011. Packaged and sold at a lower price than the original, Extra Edition game with seven additional episodes: three new, three chosen by staff from the game\\'s DLC, and one made available as a pre-order bonus. People who also owned the original game could transfer their save data between versions.\\nUnlike its two predecessors, Valkyria Chronicles III was not released in the west. According to Sega, this was due to poor sales of Valkyria Chronicles II and the general unpopularity of the PSP in the west. An unofficial fan translation patch began development in February 2012: players with a copy of Valkyria Chronicles III could download and apply the patch, which translated the game\\'s text into English. Compatible with the Extra Edition, the patch was released in January 2014.\\n= = Reception = =\\nOn its day of release in Japan, Valkyria Chronicles III topped both platform-exclusive and multi-platform sales charts. By early February, the game sold 102,779 units, coming in second overall to The Last Story for the Wii. By the end of the year, the game had sold just over 152,500 units.\\nFamitsu enjoyed the story, and were particularly pleased with the improvements to gameplay. Japanese gaming site Game Watch Impress, despite negatively noting its pacing and elements recycled from previous games, was generally positive about its story and characters, and found its gameplay entertaining despite off-putting difficulty spikes. 4Gamer.net writer Naohiko Misuosame, in a \"Play Test\" article based on the game\\'s PSN demo, felt that Valkyria Chronicles III provided a \"profound feeling of closure\" for the Valkyria Chronicles series. He praised its gameplay despite annoying limitations to aspects such as special abilities, and positively noted its shift in story to a tone similar to the first game.\\nPlayStation Official Magazine - UK praised the story\\'s blurring of Gallia\\'s moral standing, art style, and most points about its gameplay, positively noting the latter for both its continued quality and the tweaks to balance and content. Its one major criticism were multiple difficulty spikes, something that had affected the previous games. Heath Hindman of gaming website PlayStation Lifestyle praised the addition of non-linear elements and improvements or removal of mechanics from Valkyria Chronicles II in addition to praising the returning gameplay style of previous games. He also positively noted the story\\'s serious tone. Points criticized in the review were recycled elements, awkward cutscenes that seemed to include all characters in a scene for no good reason, pacing issues, and occasional problems with the game\\'s AI.\\nIn a preview of the TGS demo, Ryan Geddes of IGN was left excited as to where the game would go after completing the demo, along with enjoying the improved visuals over Valkyria Chronicles II. Kotaku\\'s Richard Eisenbeis was highly positive about the game, citing is story as a return to form after Valkyria Chronicles II and its gameplay being the best in the series. His main criticisms were its length and gameplay repetition, along with expressing regret that it would not be localized.\\n= = Legacy = =\\nKurt and Riela were featured in the Nintendo 3DS crossover Project X Zone, representing the Valkyria series. Media.Vision would return to the series to develop Valkyria: Azure Revolution, with Ozawa returning as director. Azure Revolution is a role-playing video game for the PlayStation 4 that forms the beginning of a new series within the Valkyria franchise.\\n= = = Adaptations = = =\\nValkyria Chronicles 3 was adapted into a two-episode original video animation series in the same year of its release. Titled Senjō no Valkyria 3: Taga Tame no Jūsō (戦場のヴァルキュリア３ 誰がための銃瘡, lit. Valkyria of the Battlefield 3: The Wound Taken for Someone\\'s Sake), it was originally released through PlayStation Network and Qriocity between April and May 2011. The initially-planned release and availability period needed to be extended due to a stoppage to PSN during the early summer of that year. It later released for DVD on June 29 and August 31, 2011, with separate \"Black\" and \"Blue\" editions being available for purchase. The anime is set during the latter half of Valkyria Chronicles III, detailing a mission by the Nameless against their Imperial rivals Calamity Raven. The anime was first announced in November 2010. It was developed by A-1 Pictures, produced by Shinji Motoyama, directed by Nobuhiro Kondō, and written by Hiroshi Ōnogi. Sakimoto\\'s music for the game was used in the anime.\\nThe anime\\'s title was inspired by the principle purpose of the Nameless: to suffer in battle for the goals of others. A subtitle attached to the project during development was \"The Road to Kubinka\", which referenced the Kubinka Tank Museum in Moscow. The game\\'s main theme was how the characters regained their sense of self when stripped of their names and identities, along with general themes focused on war and its consequences. While making the anime, the production team were told by Sega to make it as realistic as possible, with the consequence that the team did extensive research into aspects such as what happened when vehicles like tanks were overturned or damaged. Due to it being along the same timeline as the original game and its television anime adaptation, the cast of Valkyria Chronicles could make appearances, which pleased the team. The opening theme, \"Akari (Light) -Tomoshibi-\" (灯-TOMOSHIBI-), was sung by Japanese singer Faylan. The ending theme, \"Someday the Flowers of Light Will Bloom\" (いつか咲く光の花, Itsuka Saku Hikari no Hana), was sung by Minami Kuribayashi. Both songs\\' lyrics were written by their respective artists.\\nTwo manga adaptations were produced, following each of the game\\'s main female protagonists Imca and Riela. They were Senjō no Valkyria 3: Namo naki Chikai no Hana (戦場のヴァルキュリア3 名もなき誓いの花, lit. Valkyria of the Battlefield 3: The Flower of the Nameless Oath), illustrated by Naoyuki Fujisawa and eventually released in two volumes after being serialized in Dengeki Maoh between 2011 and 2012; and Senjō no Valkyria 3: -Akaki Unmei no Ikusa Otome- (戦場のヴァルキュリア3 -赤き運命の戦乙女-, lit. Valkyria of the Battlefield 3 -The Valkyrie of the Crimson Fate), illustrated by Mizuki Tsuge and eventually released in a single volume by Kadokawa Shoten in 2012.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset['text'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyjpARImgGbS"
      },
      "source": [
        "<center>\n",
        "<h1>Preparing Dataset for Text Completion</h1>\n",
        "</center>\n",
        "\n",
        "There are essentially 3 steps to achieve text completion.\n",
        "\n",
        "\n",
        "### 1. Start with a sentence\n",
        "Take your sentence and break it into tokens (or words), for example:\n",
        "```\n",
        "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "```\n",
        "### 2. Create progressive input output pairs\n",
        "* Start with just the first word and make the next word the \"output\" you're trying to predict:\n",
        "```\n",
        "Input: [\"The\"] → Output: \"cat\"\n",
        "```\n",
        "* Now, add one more word to the input and make the next word the output:\n",
        "```\n",
        "Input: [\"The\", \"cat\"] → Output: \"sat\"\n",
        "```\n",
        "* Keep adding more words to the input until you've gone through the sentence:\n",
        "```\n",
        "Input: [\"The\", \"cat\", \"sat\"] → Output: \"on\"\n",
        "Input: [\"The\", \"cat\", \"sat\", \"on\"] → Output: \"the\"\n",
        "Input: [\"The\", \"cat\", \"sat\", \"on\", \"the\"] → Output: \"mat\"\n",
        "```\n",
        "\n",
        "### 3. Pad the input to a fixed length\n",
        "To make all the inputs the same length (because some sentences might be shorter or longer), you \"pad\" the inputs by adding zeros at the beginning. For example, if you want every input to be 6 words long, the input would look like this:\n",
        "```\n",
        "Input: [0, 0, 0, \"The\"] → Output: \"cat\"\n",
        "Input: [0, 0, \"The\", \"cat\"] → Output: \"sat\"\n",
        "```\n",
        "This padding makes sure every input is the same size, which is important for training the model.\n",
        "\n",
        "### In Summary:\n",
        "* **Input**: You take a growing part of the sentence, starting small and getting bigger.\n",
        "* **Output**: The next word in the sentence.\n",
        "* **Padding**: If the input is too short, you add zeros at the start to make all inputs the same length.\n",
        "\n",
        "By doing this for each sentence in your dataset, you create many input-output pairs for the model to learn from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOQen8mQG0Hx"
      },
      "source": [
        "# Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qye5TrrtLOeC"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "As you can see, there are a lot of ambiguous characters such as:\n",
        "```\n",
        "\"= Valkyria Chronicles III =\\nSenjō, \"戦場のヴァルキュリア3\".\n",
        "```\n",
        "Its important to consider cleaning them.<br>\n",
        "Complete the following `clean_text()` method to build a function to clean the texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kC18d3p16p-u"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "import re\n",
        "\n",
        "# Function to clean the dataset\n",
        "def clean_text(texts):\n",
        "    \"\"\"\n",
        "    Cleans the input text by performing necessary preprocessing steps like lowercasing,\n",
        "    removing special characters, etc.\n",
        "    \"\"\"\n",
        "    cleaned_texts = []\n",
        "    for text in texts:\n",
        "        # Lowercase text\n",
        "        text = text.lower()\n",
        "        # Remove headers/formatting (e.g., '= Valkyria Chronicles III =')\n",
        "        text = re.sub(r'=+\\s*([^=]+?)\\s*=+', r'\\1', text)\n",
        "        # Remove non-alphabetic characters (punctuation, numbers, special characters)\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        # Remove extra spaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Only keep non-empty cleaned lines\n",
        "        if text:\n",
        "            cleaned_texts.append(text)\n",
        "\n",
        "    return cleaned_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6loJfl53wfs0"
      },
      "source": [
        "\n",
        "## Train, val, test split\n",
        "Out of 23k training records, you are asked to **consider atleast 5k records for training**.\n",
        "\n",
        "* **Training samples:** ~5000\n",
        "\n",
        "* **Valid samples:** 60\n",
        "\n",
        "* **Test samples:** 62\n",
        "\n",
        "Adjust the following `train_samples` variable to select the number of training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xM1dNKgi8vtF"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "# Adjust this variable to select the number of training samples\n",
        "train_samples = 5000\n",
        "\n",
        "clean_train_dataset = clean_text(train_dataset['text'][:train_samples])\n",
        "clean_valid_dataset = clean_text(valid_dataset['text'])\n",
        "clean_test_dataset = clean_text(test_dataset['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9zyqmg0eXAI"
      },
      "source": [
        "## Tokenizing and Padding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0Pq684L6HPIS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKWzxTrcBE42"
      },
      "source": [
        "Apply tokenization and convert all texts to sequences in train, valid and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OseQtmP83Xw5"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "train_sequences =tokenizer.texts_to_sequences(clean_train_dataset)\n",
        "valid_sequences =tokenizer.texts_to_sequences(clean_valid_dataset)\n",
        "test_sequences =tokenizer.texts_to_sequences(clean_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCkoKE2def-i"
      },
      "source": [
        "This is one of the crucial parameters for training. You can adjust the `max_seq_length` to determine the maximum length of the number of words in each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Eo_cT7At9yWY"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "# Set maximum sequence length (you can adjust this)\n",
        "max_seq_length = 550\n",
        "\n",
        "# Truncate the length of each sequence upto max_seq_length\n",
        "train_trunc_sequences = truncate_sequences(train_sequences, max_seq_length)\n",
        "valid_trunc_sequences = truncate_sequences(valid_sequences, max_seq_length)\n",
        "test_trunc_sequences = truncate_sequences(test_sequences, max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "teIp5ozp8-JP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sequence length: 550\n",
            "Average sequence length: 527.016820184221\n"
          ]
        }
      ],
      "source": [
        "sequence_lengths = [len(seq) for seq in train_trunc_sequences]\n",
        "print(f\"Max sequence length: {max(sequence_lengths)}\")\n",
        "print(f\"Average sequence length: {sum(sequence_lengths) / len(sequence_lengths)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayz3TMHhh1X"
      },
      "source": [
        "## Creating Input Output pairs\n",
        "The following method is used to create the input output pairs.\n",
        "```\n",
        "create_input_output_pairs(sequences, max_seq_length):\n",
        "    \"\"\"\n",
        "    Creates input-output pairs from the tokenized sequences. The input will be\n",
        "    subsequences of the original sequence (up to max_seq_length), and the output\n",
        "    will be the next token in the sequence.\n",
        "    \n",
        "    Args:\n",
        "        sequences (List[List[int]]): A list of tokenized sequences.\n",
        "        max_seq_length (int): The maximum sequence length for truncation.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Array of padded input sequences.\n",
        "        np.array: Array of output words (next token in the sequence).\n",
        "    \"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O9eKkvFb-EZ8"
      },
      "outputs": [],
      "source": [
        "# Create input-output pairs for training\n",
        "\n",
        "train_inputs, train_outputs = create_input_output_pairs(train_trunc_sequences, max_seq_length)\n",
        "valid_inputs, valid_outputs = create_input_output_pairs(valid_trunc_sequences, max_seq_length)\n",
        "test_inputs, test_outputs = create_input_output_pairs(test_trunc_sequences, max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzoC2j1UG6Ni"
      },
      "source": [
        "# Model Training and Evaluation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJCU0TEaMurY"
      },
      "source": [
        "## Model Building\n",
        "Build a `SimpleRNN` model, add hidden layers and an output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "t7j5oMxrMZdU"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,SimpleRNN,Dense\n",
        "\n",
        "def build_rnn_model(vocab_size, max_seq_length):\n",
        "    \"\"\"\n",
        "    Builds an RNN model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
        "        max_seq_length (int): Maximum input sequence length.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Compiled RNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define RNN model\n",
        "    model = Sequential([\n",
        "        # Embedding layer\n",
        "        Embedding(input_dim=vocab_size, output_dim=100, input_length=max_seq_length),\n",
        "        SimpleRNN(64),\n",
        "        # Hidden Layers\n",
        "        Dense(32, activation='relu'),\n",
        "        # Dense layer for output\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    test_model_structure(model, vocab_size)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyKyv8CYWwsX"
      },
      "source": [
        "### Improvement Strategies\n",
        "\n",
        "Here are some techniques to make the training process more robust and improve performance:\n",
        "\n",
        "* Increase the model capacity by stacking multiple **SimpleRNN** layers.\n",
        "* Change the Embedding size.\n",
        "* Consider adding `Dropout` or `BatchRegularization` layers to converge faster and generalize well.\n",
        "* Increase the dataset size or add more training data. (**Remember:** You can add upto 23k training records)\n",
        "  * You can also consider increasing the `max_seq_length`.\n",
        "* Try different optimization techniques.\n",
        "* Increase the number of epochs.\n",
        "\n",
        "**Test case:** Achieve atleast 25% validation accuracy in order to pass this test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vlSwdSrJDWLv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arj64\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The output layer must have 224884 units (matching the vocab size), but it has 1 units.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m   test_validation_accuracy(history)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 23\u001b[0m   main()\n",
            "Cell \u001b[1;32mIn[20], line 7\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      5\u001b[0m   vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Add 1 for the padding token\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m   model \u001b[38;5;241m=\u001b[39m build_rnn_model(vocab_size, max_seq_length)\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m   model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "Cell \u001b[1;32mIn[19], line 31\u001b[0m, in \u001b[0;36mbuild_rnn_model\u001b[1;34m(vocab_size, max_seq_length)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Define RNN model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Embedding layer\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     Embedding(input_dim\u001b[38;5;241m=\u001b[39mvocab_size, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mmax_seq_length),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m ])\n\u001b[1;32m---> 31\u001b[0m test_model_structure(model, vocab_size)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[1;32mc:\\Users\\arj64\\OneDrive\\Documents\\GitHub\\GEN-AI\\Labs\\Week_5\\RNN\\tests.py:47\u001b[0m, in \u001b[0;36mtest_model_structure\u001b[1;34m(model, vocab_size)\u001b[0m\n\u001b[0;32m     45\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_layer, Dense) \u001b[38;5;129;01mor\u001b[39;00m output_layer\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m!=\u001b[39m vocab_size:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe output layer must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m units (matching the vocab size), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut it has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_layer\u001b[38;5;241m.\u001b[39munits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m units.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: The output layer must have 224884 units (matching the vocab size), but it has 1 units."
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "\n",
        "def main():\n",
        "\n",
        "  vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
        "\n",
        "  model = build_rnn_model(vocab_size, max_seq_length)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # Print model summary\n",
        "  model.summary()\n",
        "  # Train the model\n",
        "  history =model.fit(train_inputs,train_outputs,epochs=20,batch_size=64, validation_split=0.2)\n",
        "\n",
        "  # Test the model on test set\n",
        "  loss, accuracy =model.evaluate(test_inputs,test_outputs)\n",
        "  print(f'\\n\\nTest Accuracy: {accuracy}')\n",
        "\n",
        "  test_validation_accuracy(history)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKnNvnp4qihb"
      },
      "source": [
        "## BLEU Score Evaluation\n",
        "\n",
        "BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate how well a machine-generated text matches a human-written reference text. It's commonly used in tasks like machine translation and text generation. <br> The score ranges from 0 to 1.\n",
        "* 1 means the generated text perfectly matches the reference.\n",
        "* 0 means there's no similarity at all.\n",
        "\n",
        "It checks for both word matches and the correct sequence of words, while penalizing texts that are too short or too long.\n",
        "\n",
        "In order to perform this evaluation,\n",
        "1. We'll be converting the tokens back to words and get the reference words.\n",
        "2. Generate predictions on the input tokens and get the predicted words.\n",
        "3. Then calculate BLEU score by passing the reference and predicted words as input.\n",
        "\n",
        "**Test case**: Achieve Atleast 15% to pass this test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YvmZMT4GG8r"
      },
      "outputs": [],
      "source": [
        "# Convert test outputs to reference words\n",
        "reference_words = convert_token_ids_to_words(test_outputs, tokenizer)\n",
        "\n",
        "# Generate predictions\n",
        "predicted_words = generate_predictions(model, test_inputs, tokenizer)\n",
        "\n",
        "# Calculate BLEU score\n",
        "calculate_bleu(predicted_words, reference_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N5JdEMTGGwh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKd2l64g1huN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFqMGdcY1hsU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFGB7tZz1hny"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2AxYY6DVqu4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
